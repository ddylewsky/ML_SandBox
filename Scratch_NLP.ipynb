{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of NLP using Bigrams and Trigrams\n",
    "Here we are going to programmatically generate some Data Science content using bigrams and trigrams. First by sourcing a corpus of Data Science documents from the web by scraping a data science article on oreilly.com. Next we will clean and prepare the data for our bigram and trigram algorithms. \n",
    "\n",
    "This is a notebook based on the NLP chapter in \"Data Science from Scratch\" by Joel Grus. For more examples or information please check out the book, which I recommend, here:\n",
    "https://learning.oreilly.com/library/view/data-science-from/9781492041122/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use the Requests and Beautiful soup libraries to get our corpus of documents from an essay written by Mike Loukides titled \"What is Data Science\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from collections import defaultdict\n",
    "\n",
    "url = \"https://www.oreilly.com/ideas/what-is-data-science\"\n",
    "html = requests.get(url).text\n",
    "soup = BeautifulSoup(html, 'html5lib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look a the html data, the article body can be accessed by finding the div with the class \"main-post-radar-content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = soup.find(\"div\", \"main-post-radar-content\")   # find article-body div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and Prep the Data\n",
    "As with most data sourced this way it is far from clean. So lets do some basic cleaning. First, we will clean the apostrophes that are character represented as \"â\" and replace them with normal apostrophes with a function called fix_apostrophes. \n",
    "\n",
    "Next we will want to create a sequence (list) of words and use the \".\" as a way of marking where each sentence ends.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_apostrophes(text: str) -> str:\n",
    "    return text.replace(\"â\", \"'\")\n",
    "\n",
    "regex = r\"[\\w']+|[\\.]\"                      \n",
    "document = []\n",
    "\n",
    "for paragraph in content(\"p\"):\n",
    "    words = re.findall(regex, fix_apostrophes(paragraph.text))\n",
    "    document.extend(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"We'\",\n",
       " 've',\n",
       " 'all',\n",
       " 'heard',\n",
       " 'it',\n",
       " 'according',\n",
       " 'to',\n",
       " 'Hal',\n",
       " 'Varian',\n",
       " 'statistics']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets create bigram model\n",
    "This is a model that, given some starting word, will randomly choose one of the words that follow it in the corpus. This is done by using a defaultdict to create a lookup of words that follow a given target word. Once we have this lookup we can randomly select one of the values in the list of words that follow our target word. When the model encounters a \".\" it will stop and return the string of content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = defaultdict(list)\n",
    "for prev, current in zip(document, document[1:]):\n",
    "    transitions[prev].append(current)\n",
    "    \n",
    "def bigrams_model() -> str:\n",
    "    current = \".\"\n",
    "    result = []\n",
    "    while True:\n",
    "        next_word_candidates = transitions[current]\n",
    "        current = random.choice(next_word_candidates)\n",
    "        result.append(current)\n",
    "        if current == \".\": return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asking things change over time . -Bigram Model\n"
     ]
    }
   ],
   "source": [
    "bi_gram_text = bigrams_model()\n",
    "print(bi_gram_text, '-Bigram Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try using trigrams \n",
    "Trigrams are triplets of consecutive words. Lets see if they create sentences that make a little more sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_transitions = defaultdict(list)\n",
    "starts = []\n",
    "\n",
    "for prev, current, next in zip(document, document[1:], document[2:]):\n",
    "\n",
    "    if prev == \".\":              \n",
    "        starts.append(current) \n",
    "\n",
    "    trigram_transitions[(prev, current)].append(next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigrams_model() -> str:\n",
    "    current = random.choice(starts)   \n",
    "    prev = \".\"                        \n",
    "    result = [current]\n",
    "    while True:\n",
    "        next_word_candidates = trigram_transitions[(prev, current)]\n",
    "        next_word = random.choice(next_word_candidates)\n",
    "\n",
    "        prev, current = current, next_word\n",
    "        result.append(current)\n",
    "\n",
    "        if current == \".\":\n",
    "            return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"' Almost any e commerce application is a great example of data jiujitsu identifying music by analyzing an audio stream directly is a quintessential artificial intelligence with human intelligence .\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tri_gram_text = trigrams_model()\n",
    "tri_gram_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
